{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNXyKBC5FEB5FJxdyfsv0R+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkstar444/Telcom-Customer-Churn-Dataset-Analysis/blob/main/Telcom_Customer_Churn_Dataset_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "zzqJDcAA7BNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ef5aa0"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "Load the Telco Customer Churn dataset into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ab30538"
      },
      "source": [
        "Load the data into a pandas DataFrame and display the first few rows and information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "470cec13"
      },
      "source": [
        "df = pd.read_csv('/content/Telco_Cusomer_Churn.csv')\n",
        "display(df.head())\n",
        "display(df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8afde5d4"
      },
      "source": [
        "## Explore the data\n",
        "\n",
        "\n",
        "Perform exploratory data analysis (EDA) to understand the data distribution, identify missing values, and visualize relationships between features and the target variable (Churn).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7022dce"
      },
      "source": [
        "\n",
        "Display descriptive statistics for numerical columns, check for missing values, and visualize the distribution of the 'Churn' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58d3449a"
      },
      "source": [
        "display(df.describe())\n",
        "display(df.isnull().sum())\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Churn', data=df)\n",
        "plt.title('Distribution of Churn')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c1ea699"
      },
      "source": [
        "Visualize the distribution of relevant categorical features and their relationship with 'Churn' using count plots.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "803de721"
      },
      "source": [
        "categorical_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
        "plt.figure(figsize=(16, 12))\n",
        "for i, col in enumerate(categorical_features):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    sns.countplot(x=col, hue='Churn', data=df)\n",
        "    plt.title(f'Distribution of {col} by Churn')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "982f199b"
      },
      "source": [
        "Convert 'TotalCharges' to a numerical type and then visualize the distributions of numerical features using histograms and the relationship between numerical features and 'Churn' using box plots.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "219f8851"
      },
      "source": [
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "df.dropna(inplace=True)\n",
        "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, col in enumerate(numerical_features):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    sns.histplot(x=col, data=df, kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, col in enumerate(numerical_features):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    sns.boxplot(x='Churn', y=col, data=df)\n",
        "    plt.title(f'{col} by Churn')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c2ebf30"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "Handle missing values, encode categorical features, and scale numerical features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "333c7242"
      },
      "source": [
        "Identify categorical and numerical columns, apply one-hot encoding to categorical columns, apply standard scaling to numerical columns, encode the target variable, and concatenate the processed features into a single DataFrame as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b70c9421"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_features.remove('Churn')\n",
        "categorical_features.remove('customerID')\n",
        "\n",
        "numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n",
        "numerical_features.remove('SeniorCitizen') # SeniorCitizen is treated as categorical\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Create a column transformer to apply different transformations to different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Apply preprocessing\n",
        "X = preprocessor.fit_transform(df.drop(['Churn', 'customerID'], axis=1))\n",
        "\n",
        "# Encode the target variable\n",
        "y = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "# Convert the processed features back to a DataFrame (optional, but good for inspection)\n",
        "# Get feature names after one-hot encoding\n",
        "ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "all_feature_names = numerical_features + list(ohe_feature_names)\n",
        "\n",
        "X_processed = pd.DataFrame(X, columns=all_feature_names, index=df.index)\n",
        "\n",
        "display(X_processed.head())\n",
        "display(y.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60349e2b"
      },
      "source": [
        "## Split the data\n",
        "\n",
        "Split the dataset into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8876ff45"
      },
      "source": [
        "Split the preprocessed data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772e1fdd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24db13a9"
      },
      "source": [
        "## Build and train models\n",
        "\n",
        "Choose appropriate machine learning models for churn prediction and train them on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47d00b7"
      },
      "source": [
        "Import the necessary classification models, instantiate them, and train them on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b39926be"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Instantiate the models\n",
        "log_reg_model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Train the models\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "dt_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Models trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a25f2229"
      },
      "source": [
        "## Evaluate the models\n",
        "\n",
        "Evaluate the performance of the trained models using appropriate metrics (e.g., accuracy, precision, recall, F1-score, AUC).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d5161a4"
      },
      "source": [
        "Import the necessary metrics from sklearn.metrics and then iterate through the trained models to make predictions and calculate evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa9684f8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "models = [log_reg_model, dt_model, rf_model, gb_model]\n",
        "model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting']\n",
        "\n",
        "for model, name in zip(models, model_names):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(\"-\" * (len(name) + 6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b928f09e"
      },
      "source": [
        "## Interpret model results\n",
        "\n",
        "Analyze the model results to understand which features are important for predicting churn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47f63f79"
      },
      "source": [
        "\n",
        "Extract and display feature importances for tree-based models and coefficients for Logistic Regression to understand which features influence churn prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b35266e4"
      },
      "source": [
        "# Get feature names from X_processed\n",
        "feature_names = X_processed.columns\n",
        "\n",
        "# Feature importances for Decision Tree\n",
        "dt_importances = pd.Series(dt_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "print(\"--- Decision Tree Feature Importances (Top 10) ---\")\n",
        "print(dt_importances.head(10))\n",
        "print(\"-\" * 45)\n",
        "\n",
        "# Feature importances for Random Forest\n",
        "rf_importances = pd.Series(rf_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "print(\"\\n--- Random Forest Feature Importances (Top 10) ---\")\n",
        "print(rf_importances.head(10))\n",
        "print(\"-\" * 46)\n",
        "\n",
        "# Feature importances for Gradient Boosting\n",
        "gb_importances = pd.Series(gb_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "print(\"\\n--- Gradient Boosting Feature Importances (Top 10) ---\")\n",
        "print(gb_importances.head(10))\n",
        "print(\"-\" * 49)\n",
        "\n",
        "# Coefficients for Logistic Regression\n",
        "# The magnitude of the coefficient indicates the strength of the relationship, and the sign indicates the direction.\n",
        "log_reg_coef = pd.Series(log_reg_model.coef_[0], index=feature_names).sort_values(key=abs, ascending=False)\n",
        "print(\"\\n--- Logistic Regression Coefficients (Top 10 by absolute value) ---\")\n",
        "print(log_reg_coef.head(10))\n",
        "print(\"-\" * 62)\n",
        "\n",
        "# Summarize key features across models\n",
        "print(\"\\n--- Summary of Key Features ---\")\n",
        "print(\"Features consistently important across tree-based models:\")\n",
        "# Find common top features (example: top 5 from each tree model)\n",
        "top_dt_features = set(dt_importances.head(5).index)\n",
        "top_rf_features = set(rf_importances.head(5).index)\n",
        "top_gb_features = set(gb_importances.head(5).index)\n",
        "\n",
        "common_tree_features = list(top_dt_features.intersection(top_rf_features, top_gb_features))\n",
        "print(common_tree_features)\n",
        "\n",
        "print(\"\\nFeatures with high absolute coefficients in Logistic Regression:\")\n",
        "print(log_reg_coef.head(5).index.tolist())\n",
        "\n",
        "print(\"\\nOverall influential features:\")\n",
        "# Combine and show features that appear in top lists across models\n",
        "influential_features = set(dt_importances.head(10).index)\n",
        "influential_features.update(rf_importances.head(10).index)\n",
        "influential_features.update(gb_importances.head(10).index)\n",
        "influential_features.update(log_reg_coef.head(10).index)\n",
        "\n",
        "print(list(influential_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0e97d0f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset contains 7043 entries and 21 columns, with most features being categorical.\n",
        "*   The `TotalCharges` column initially contained non-numeric values, which were handled by coercing to numeric and dropping 11 resulting rows with missing values.\n",
        "*   The dataset exhibits class imbalance, with more customers who did not churn than those who did.\n",
        "*   Customers with month-to-month contracts, no online security, or no tech support appear to have higher churn rates.\n",
        "*   Customers who churned generally had lower `tenure` and `TotalCharges`, but slightly higher `MonthlyCharges` on average.\n",
        "*   The data was successfully preprocessed by encoding categorical features using one-hot encoding and scaling numerical features using StandardScaler, resulting in 44 features.\n",
        "*   The dataset was split into training (80%, 5625 samples) and testing (20%, 1407 samples) sets.\n",
        "*   Four models (Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting) were trained for churn prediction.\n",
        "*   Model evaluation on the test set showed varying performance metrics. Gradient Boosting achieved the highest Accuracy (0.7946) and AUC (0.8358) among the evaluated models.\n",
        "*   Features consistently important across tree-based models include `TotalCharges`, `MonthlyCharges`, `tenure`, and `Contract_Month-to-month`.\n",
        "*   Logistic Regression identified `tenure`, `Contract_Two year`, `TotalCharges`, and `Contract_Month-to-month` as having the highest absolute coefficients, indicating strong relationships with churn probability.\n",
        "*   Overall influential features identified across models include `tenure`, `MonthlyCharges`, `TotalCharges`, `Contract_Month-to-month`, various internet service and security features, and payment method.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The class imbalance in the target variable should be addressed in future modeling steps using techniques like oversampling, undersampling, or using evaluation metrics more robust to imbalance (like F1-score or AUC) for model selection.\n",
        "*   Further model tuning using techniques like cross-validation and hyperparameter optimization could potentially improve the performance of the trained models, especially for the tree-based models.\n"
      ]
    }
  ]
}